# -*- coding: utf-8 -*-
"""Solving Filter spam and non spam emails case study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LCfNhoZwSTxNDiz7RD3Red4o7PU-AYVf

## Introduction

- We are given the various text files containing Spam and Non spam emails. The purpose of this prolem statement is to claasify if a email is spam or Non Spam.

- This is a supervised learning problem in which we use classification algorithms to classify the results. So now we are going to start from the bottom with real email messages and have them classified as spam and non-spam.

## **All the work that we need to do can be split in 4 steps:**

**1. Importing Libraries**

**2. Load and check data**

**3. Performing Text Cleaning**

**4. MODEL SPLITTING**

**5. Vectorizing Data**

**6. Modelling**
- Grid Search SVC
- Logistic Regression
- Decision tree classifier
- Naive Bayes Classifier

## Importing Libraries

Importing necessory python libraries that we are going to use in our further processing on data.

- **glob:** The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.

  **glob.glob(pathname)**
  
- **os:** The OS module in python provides functions for interacting with the operating system. OS, comes under Python’s standard utility modules. This module provides a portable way of using operating system dependent functionality. The *os* and *os.path* modules include many functions to interact with the file system.
**ex:** os.name, os.getcwd(), os.error etc.
 
- **sklearn:** Simple and efficient tools for data mining and data analysis. Built on NumPy, SciPy, and matplotlib.
 
 In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T) . An example of an estimator is the class sklearn.svm.SVC , which implements support vector classification.
 
- **NLTK:** NLTK is one of the leading platforms for working with human language data and Python, the module NLTK is used for **natural language processing**.

- **collections:** Collections in Python are containers that are used to store collections of data, for example, list, dict, set, tuple etc
"""

import glob
import os
from sklearn.svm import SVC
from nltk.corpus import names
from collections import defaultdict
from nltk.stem import WordNetLemmatizer

"""**- Preparing the text data**

The data-set used here, contains two folder namily spam and ham. The data-set is split into a training set and a test set containing 1500 mails and 3672 mails respectively, divided equally between spam and ham mails.

- ham— train dataset, contains 3672 non spam emails
- spam — train dataset, contains 1500 spam emails

## **Load and check data**

Mount the data from google drive so we can acces the drive data on colab
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Define empty arrays for emails and lables. In the next section we will read the email files from file path and then append values to these empty arrays."""

emails = []
labels = []

file_path_spam = '/content/gdrive/My Drive/ML Datasets/Enron Case Study/enron1/spam'
print(file_path_spam)

"""Loop through the all email files in **spam** folder, read all emails and append values to emails array.

Apppend label **"1**" to each file.
"""

for filename in glob.glob(os.path.join(file_path_spam,'*.txt')):
    with open(filename,'r',encoding = "ISO-8859-1") as infile:
        emails.append(infile.read())
        labels.append(1)
        
#print(emails)
#print(labels)

file_path_ham = '/content/gdrive/My Drive/ML Datasets/Enron Case Study/enron1/ham'

"""Loop through the all email files in **ham** folder, read all emails and append values to emails array.

Apppend label **"0"** to each file.
"""

for file in glob.glob(os.path.join(file_path_ham, "*.txt")):
    with open(filename,'r',encoding = "ISO-8859-1") as infile:
        emails.append(infile.read())
        labels.append(0)

"""## Performing Text Cleaning

**Importing nltk library**

Importing nltk library for text processing and download some of it's packages that are useful in our text processing.
"""

import nltk
nltk.download('names')
nltk.download('wordnet')

all_names = set(names.words())
WNL = WordNetLemmatizer()
#print(all_names)

"""As this is a text mining problem, text cleaning is the first step where we remove those words from the document which may not contribute to the information we want to extract.

Emails may contain a lot of undesirable characters like punctuation marks, stop words, digits, etc which may not be helpful in detecting the spam email. Removing of these characters can be done in many different ways, these are some of the possible filters that can be applied:-

- **Lower-casing**: The entire email is converted into lower case, so
that captialization is ignored (e.g., IndIcaTE is treated the same as
Indicate).

- **Stripping HTML:** All HTML tags are removed from the emails.
Many emails often come with HTML formatting; we remove all the
HTML tags, so that only the content remains.

- **Normalizing URLs:** All URLs are replaced with the text “httpaddr”.

- **Normalizing Email Addresses:** All email addresses are replaced
with the text “emailaddr”.

- **Normalizing Numbers:** All numbers are replaced with the text
“number”.

- **Normalizing Dollars:** All dollar signs ($) are replaced with the text
“dollar”.

- **Removal of non-words:** Non-words and punctuation have been removed.
All white spaces (tabs, newlines, spaces) have all been trimmed
to a single space character.

- **Removal of stop words –** Stop words like “and”, “the”, “of”, etc are very common in all English sentences and are not very meaningful in deciding spam or legitimate status, so these words have been removed from the emails.

- **Lemmatization** – It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider meaning of the sentence).

- **Stemming** – It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, “include”, “includes,” and “included” would all be represented as “include”. The context of the sentence is not preserved in stemming (another buzz word in text mining which does not consider meaning of the sentence).
"""

def clean(data):
    cleaned = defaultdict(list)
    #print(cleaned)
    count = 0
    
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1 
    return(list(cleaned.values()))

dataX = clean(emails)
dataY = labels

print(dataX)
print(dataY)

"""- To perform text cleaning we call a function which receives the data appended in the emails array as an argument.

- In this first we created an empty list then loop through the emails list.

- If the word is alphabatic and not present in word list defined by nltk library, we convert the word in lower case and then lemmatize the word.

- Finally we append all the words that we have cleaned and put in a list dataX and put labels in dataY variable.

## MODEL SPLITTING

- Split arrays or matrices into random train and test subsets

- In model splitting we split our data into training and testing set.

- From these, we want to get a test and training set of data, so we can use our **train_test_split**.

- **test_size :** float, int or None, optional (default=0.25)
If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.

- **random_state:** We provide the proportion of data to use as a test set and we can provide the parameter **random_state**, which is a seed to ensure repeatable results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size = 0.2, random_state=56)
# print(len(X_test))

"""## VECTORIZING DATA

- The **sklearn.feature_extraction** module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.

- Machines are not capable of understanding words and sentences in the same manner as humans do. In order to make documents’ corpora more palatable for computers, they must first be converted into some **numerical structure**. This process is called vectorization of data.

- There are various methods available to vectorize the data like **countVectorizer, TfidfVectorizer**. Here we are using **TfidfVectorizer**.

- **TfidfVectorizer:** The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. 

  With the TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. - This is the IDF (inverse document frequency part).

  This helps to adjust for the fact that some words appear more frequently.
  
  **formula used:** tf-idf(d, t) = tf(t) * idf(d, t)
  
  * tf(t)= the term frequency is the number of times the term appears in the document
  * idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'
  
  **Term frequency is defined as:**

  ###  **tf(t) =  N**
  
  Where N is the number of times the term appears in the document.

  **Inverse document frequency is defined as:**

  ###  **idf(t,D) = log( N / |d∈D:t∈d| )**

  Where N is the total number of documents in the corpus, and |d∈D:t∈d| is the number of documents   where t appears.
  
  
 - **stop_words:** Remove the stopwords defined of english.
 
 - **max_features:** number of maximum features selected.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer(stop_words='english', max_features=1000)
X_train_vectorized = tf.fit_transform(X_train)
X_test_vectorized = tf.transform(X_test)

"""- Here first we create object of TfidfVectorizer and then apply fit_transform method on X_train data and transform method to X_test data.

- **fit_transform and transform:** To center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation.

  ###  x ′= (x−μ) / σ
  
  We do that on the training set of data. But then we have to apply the same transformation to your     testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to   use the same two parameters μ and σ (values) that you used for centering the training set.

  Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of            StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform()   method to apply the transformation to a particular set of examples.

  fit_transform() joins these two steps and is used for the initial fitting of parameters on the training  set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the   same data.

## MODELLING

### Using Grid Search on SVC

- Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model.

- Here we model the classifier with the parameters kernel='linear'.
"""

svc_lib = SVC(kernel = 'linear')
parameters = {'C' : (0.5,1.0,10,100)}

from sklearn.model_selection import GridSearchCV
import timeit

grid_search1 = GridSearchCV(svc_lib, parameters, n_jobs = -1, cv = 3)
grid_search1.fit(X_train_vectorized, y_train)

start_time = timeit.default_timer()
final = timeit.default_timer()-start_time
print("Execution Time in Training : ",final)

print(grid_search1.best_params_)
print(grid_search1.best_score_)

grid_search_best1 = grid_search1.best_estimator_
accur1 = grid_search_best1.score(X_test_vectorized, y_test)
print("Model Accuracy: ",accur1)

"""### Using Logistic Regression

- Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split, cross_val_score

logreg = LogisticRegression(random_state=9)

logreg.fit(X_train_vectorized, y_train)

y_pred_rf = logreg.predict(X_test_vectorized)

print(classification_report(y_test, y_pred_rf))

"""### Using Naive Bayes

- This is a simple (naive) classification method based on Bayes rule. It relies on a very simple representation of the document (called the bag of words representation)

- For difference between Naive Bayes & Multinomial Naive Bayes:

  Naive Bayes is generic. Multinomial Naive Bayes is a specific instance of Naive Bayes where the P(Featurei|Class) follows multinomial distribution (word counts, probabilities, etc.)
  
- Bayes’ Rule applied to Documents and Classes

  For a document d and a class c, and using Bayes’ rule,

  ###  P( c | d ) = [ P( d | c ) x P( c ) ] / [ P( d ) ]
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_auc_score, accuracy_score

clfrNB = MultinomialNB(alpha = 0.1)
clfrNB.fit(X_train_vectorized, y_train)
preds = clfrNB.predict(X_test_vectorized)
score = roc_auc_score(y_test, preds)
print(score)

"""### Using Decision tree

- Decision tree builds classification or regression models in the form of a tree structure. 

- It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. 

- A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.
"""

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=17)
clf = clf.fit(X_train_vectorized, y_train)

y_pred = clf.predict(X_test_vectorized)
print(classification_report(y_test, y_pred))
print('\nAccuracy: {0:.4f}'.format(accuracy_score(y_test, y_pred)))

